
 
 ## Hi ðŸ‘‹ Welcome to My GitHub Profile !


An aspiring AWS Data Engineer with a strong foundation in cloud technologies and data engineering concepts. I am looking to associate myself with an esteemed organization where I can apply my technical and interpersonal skills to contribute effectively to its growth. I am committed to continuous learning and eager to take on new challenges that allow me to be resourceful, innovative, and flexible. I am particularly interested in leveraging AWS services, Python, and data pipelines to build scalable and efficient data solutions.



# Skills: 

     â€¢	Programming / Scripting : Python, Pyspark, SQL.
     â€¢	Databases : Oracle, MySQL, SQL server 
     â€¢	AWS Services : Glue, EMR,EC2, Redshift, Athena, RDS, S3,SQS, SNS.
     â€¢	Versioning tools : GitHub.
     â€¢	Operating Systems : Microsoft Windows Server 2008/12/16, Linux.
     â€¢	Tools : Visual Studio code, SQL workbench, Databricks.
     â€¢	Architecture: Microservices.


 # Project:

Online Movie Rating Pipeline:    
â€¢	I am building an ETL pipeline for a movie review platform using PySpark on Databricks.
â€¢	The platform collects user ratings and comments on movies. Raw data is extracted from CSV and JSON files stored in cloud storage .
â€¢	Using PySpark, I clean and transform the data to calculate the average rating per movie, identify top reviewers based on review count or rating quality, and handle missing or inconsistent fields.
â€¢	Finally, the cleaned data is partitioned by genre and stored in Parquet format for efficient querying and analytics on Databricks.



  Banking Transaction  ETL:  
                   I am analyzing banking transactions to detect possible fraud patterns and understand customer spending trends. Using PySpark on Databricks, I build an ETL pipeline that ingests raw  data extracted from CSV and JSON. The pipeline performs data cleaning and validation, filters out corrupted or duplicate records, and tags high-value transactions based on dynamic thresholds. It then aggregates and summarizes total spend per customer, enabling insights into spending behavior. The processed data is stored partitioned by transaction type for efficient querying and reporting in Databricks.

